#!/usr/bin/env python
"""Generate pseudo 'gold' labels from an existing classified predictions file.

This is ONLY for demonstration when no human labels exist. It biases evaluation
upwards and must not be treated as a real benchmark.

Strategy:
  1. Load predictions (requires fields: text, label, confidence).
  2. Filter by min confidence per class (default 0.75) to reduce noise.
  3. Cap per-class count (balanced sampling) via --per-class.
  4. Optional diversity: simple dedupe by lowercase text; future: embedding distance.
  5. Emit JSONL in annotation schema format with annotator='AUTO' and sample_phase='pseudo'.

Usage:
  python scripts/pseudo_label_generate.py --pred out/insights_classified.jsonl \
    --out data/pseudo_gold.jsonl --min-conf 0.75 --per-class 40
"""
import argparse, json, random
from pathlib import Path
from collections import defaultdict

LABELS = ["Advantage","Risk","Neutral"]

SCHEMA_BASE = {
    'id': None,
    'text': None,
    'label': None,
    'sourceUrl': None,
    'candidateType': None,
    'qualityScore': None,
    'provenance': 'scraped',
    'sample_phase': 'pseudo',
    'annotator': 'AUTO',
    'taxonomyVersion': 'v1.0-draft',
    'rationale_gold': None,
    'notes': 'SYNTHETIC_AUTOGENERATED'
}

def iter_jsonl(p: Path):
    with p.open('r', encoding='utf-8') as f:
        for line in f:
            line=line.strip();
            if not line: continue
            try: yield json.loads(line)
            except: continue


def main():
    ap=argparse.ArgumentParser()
    ap.add_argument('--pred', required=True)
    ap.add_argument('--out', required=True)
    ap.add_argument('--min-conf', type=float, default=0.75)
    ap.add_argument('--per-class', type=int, default=40)
    ap.add_argument('--seed', type=int, default=42)
    args=ap.parse_args()

    random.seed(args.seed)
    rows=list(iter_jsonl(Path(args.pred)))
    buckets=defaultdict(list)
    for r in rows:
        lab=r.get('label'); conf=r.get('confidence',0.0)
        if lab in LABELS and conf >= args.min_conf:
            buckets[lab].append(r)

    # Shuffle and cap
    out_rows=[]
    for lab in LABELS:
        candidates=buckets.get(lab, [])
        random.shuffle(candidates)
        for i,c in enumerate(candidates[:args.per_class]):
            rec=dict(SCHEMA_BASE)
            rec.update({
                'id': len(out_rows),
                'text': c.get('text'),
                'label': lab,
                'sourceUrl': c.get('sourceUrl'),
                'candidateType': None,
                'qualityScore': c.get('confidence'),
            })
            out_rows.append(rec)

    out_path=Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open('w', encoding='utf-8') as w:
        for r in out_rows:
            w.write(json.dumps(r, ensure_ascii=False)+'\n')

    summary={
        'input': str(Path(args.pred).resolve()),
        'output': str(out_path.resolve()),
        'counts': {lab: sum(1 for r in out_rows if r['label']==lab) for lab in LABELS},
        'total': len(out_rows),
        'min_conf': args.min_conf,
        'per_class_cap': args.per_class,
        'note': 'SYNTHETIC PSEUDO-GOLD; DO NOT USE FOR FINAL BENCHMARK',
    }
    print(json.dumps(summary, indent=2))

if __name__=='__main__':
    main()
